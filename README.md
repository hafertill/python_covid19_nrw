# Automated workflow for data scraping

## Process
- Get data from url
- clean data in pandas dataframe
- write dataframe to .csv
- store .csv in s3-bucket as text-file
- deploy scraper to aws lambda and run it there every 10 minutes

## Contributing
- Make pull request (or ask for repo membership)
- After review your new scraper will be deployed automatically

## To Dos
- automate deploy

## How to test locally
-

## Automated tests
-